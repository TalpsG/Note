# 机器学习的基本概念
## 函数的分类

`regression`: 输入一些数值，输出一个数值
`classification`：输出类别
`structured learning`： 生成文章，图像等

## 如何找到函数
- 先确定一个函数方程，比如一次函数，二次函数等
- 给出函数参数的初值，根据一些背景知识
  - 参数也分为`weight`和`bias`
- 定义`loss`损失,用于衡量这个函数的预测结果的效果
  - `MSE` `MAE` 等
- `optimization`：让`loss`最小
  - 梯度下降,容易陷入局部最优
  - 学习速率，步长  

## liner model
线性模型太过简单，不能拟合很多场景。

## sigmoid
使用`sigmoid`来生成许多线段，然后将线段叠加去拟合曲线。

这也就是需要激活函数的原因。

![sigmoid](./img/sigmoid.png) 


矩阵乘法
![matirx](./img/matrix.png) 

最终的函数形式改写为矩阵的形式
![final](./img/final.png) 


## 实际的训练过程
把训练数据分成`batch`,然后算每个`batch`的`loss`然后梯度下降调整参数，对所有`batch`都做完上述操作叫做一个`epoch`。


## 为什么deep而不是fat？


## 回顾
机器学习的框架
1. 设计网络(带未知数的函数)
2. 定义损失
3. 优化


损失很大的原因
1. 模型的表达能力不够
2. 优化做得不好
  - 比如梯度下降陷入局部最优


如何确定原因？
可以从小网络到大网络，如果大网络的loss比小网络还高，那很可能是优化的问题。

## small-gradient

### 网络的loss降不下来
局部最优和鞍点。

如何了解自己的网络是否卡在了这两种点上？

其实就是去看这个点的附近区域与该点的关系



### batch
`batch`会影响训练的速度以及优化的结果

`batch`过小:
1. 训练速度不一定快，可能没有充分利用到并行计算的优势
2. 优化更新频率高，单次计算梯度速度快
3. 单次`epoch`时间长

`batch`过大：
1. 训练速度不一定慢，因为可以利用并行计算这一硬件特点
2. 优化更新频率低，单次计算速度慢
3. 单次`epoch`时间短


小`batch`得到的曲线会更平滑，泛用型会更好


### momentum
冲量是模拟现实世界的惯性，在训练时为了避免陷入局部最优引入了冲量这一个概念。
我们在更新参数时会加权上上一次更新的向量。


### optimizer
当训练过程中发生`loss`波动小的时候，可能是因为参数向量一直在`critial point`处徘徊导致的。

这个问题跟学习率有关，当学习率过高，可能会导致曲线震荡，而过低又有可能由于梯度趋于0,导致更新速度过慢。而且在训练时两种情况可能都会发生，一个固定的学习率不能很好的适应两种情况。

`critical point`就是梯度为0的点。


这时需要其他的信息来加权学习率

真正的学习率=  加权 分之 设置的学习率
#### RMS均方根法
学习率加权 = 所有之前的梯度的平方和的平均值开根号 
这样可以让梯度大的损失曲线步长慢，小的步长大。

#### RMSProp
学习率的加权 和上一次的加权有关 

学习率加权= (alpha * 本次要更新的梯度平方和  + (1-alpha) * 上一次更新的加权  ) 开根号

这样的好处是最近的学习率加权影响比过去的大。


`Adam`的优化法就是 `RMSProp`加上`Momentum`


#### 学习率设置
可以根据时间设置，时间越长学习率越小。
也可以`warmup`，在开始的一段时间内学习率越来越大，之后越来越小。

### 分类
可以使用线性回归的方法，只不过输出多个输出，然后对 将输出结果归一化(softmax)。

交叉熵损失函数要比`mean square error`要更好。
1. 当结果接近于0或1的时候，`mse`的梯度会变的很小，更新速度会很慢。而交叉熵损失函数有着更好的梯度特性。


### 一些tricks
`dropout`:防止过拟合导致的验证效果不好，可以用`dropout`将神经元输出随即丢弃一部分
`batchnorm`:
- 允许使用更高的学习率，加速训练。
- 对参数初始值不那么敏感。
- 提供一定程度的正则化效果，有助于防止模型过拟合。
- 可以加速收敛，有助于优化深层网络
