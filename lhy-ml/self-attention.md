## 注意力
输入不定，输出与输入长度一样。

每一个输出考虑所有的输入


对应输入的输出要考虑与自己输入相关的其他输入。


如何计算输入间的相关性？
1. `dot product`
2. `additive`


计算完的结果后会用`softmax`归一一下得到了真正的每个输入之间的相关性。

每个输入对应的输出实际上就是各个输入通过矩阵的结果加权(成上相关性)加在一起而已。


`query`:用来计算相关性的
`key`：也是用来计算相关性的,本输入的`k`和其他输入的`q`进行点乘可以得到与其他输入相关性。
`value`:用来计算输出的

每一个输入都会计算以上三个数据


## multi-head
将相关性分为多个，要计算输入之间的多个相关性。
因此`q v k`都有多个。最终一个输入会得到多个输出，再乘上一个矩阵后会得到最终的对应输出。



## positional encoding
常规的注意力没有利用位置信息。

可以将输入加上一个带有位置信息的矩阵。


## truncated attention
对输入的数量很多时，我们可以减少注意力机制的输入个数，加快处理速度。


## self-attention vs CNN
`cnn`可以看作简化版的`self-attention`

## self-attention vs rnn
`rnn`是串行的结构，而`self-attention`是并行的。
### rnn
输入是一个序列，前一个输入的输出会和当前的输入一起输入到`RNN`中计算下一个输出。
